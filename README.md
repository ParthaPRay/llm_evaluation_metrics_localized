# llm_metrics_local
This repo contains codes local llm metrics evaluation

# Metrics for Evaluating LLM Efficiency

This document provides a detailed explanation of the metrics used to evaluate the efficiency and performance of large language models (LLMs). The metrics are divided into **existing metrics** derived from raw data and **novel derived metrics** introduced to enhance evaluation capabilities.

---

## **Existing Metrics**

### 1. **Total Duration**
- **Definition**: The total time spent on processing, including model loading, prompt evaluation, and token generation.
- **Unit**: Nanoseconds (ns)
- **Purpose**: Measures the overall latency for the complete process.

### 2. **Load Duration**
- **Definition**: The time taken to load the model into memory.
- **Unit**: Nanoseconds (ns)
- **Purpose**: Evaluates the overhead associated with model initialization.

### 3. **Prompt Evaluation Count**
- **Definition**: The number of tokens in the input prompt that the model processes.
- **Unit**: Count
- **Purpose**: Indicates the complexity or size of the input prompt.

### 4. **Prompt Evaluation Duration**
- **Definition**: The time spent processing the input prompt.
- **Unit**: Nanoseconds (ns)
- **Purpose**: Measures the efficiency of the model in analyzing the prompt.

### 5. **Evaluation Count**
- **Definition**: The number of tokens generated in the response.
- **Unit**: Count
- **Purpose**: Reflects the length of the output generated by the model.

### 6. **Evaluation Duration**
- **Definition**: The time taken to generate the tokens in the response.
- **Unit**: Nanoseconds (ns)
- **Purpose**: Measures the efficiency of token generation.

### 7. **Tokens per Second (Tokens/s)**
- **Definition**: The speed at which tokens are generated.
- **Formula**:
  
  \[
  \text{Tokens/s} = \frac{\text{Evaluation Count}}{\text{Evaluation Duration} \times 10^{-9}}
  \]
- **Purpose**: Benchmarks the raw speed of token generation.

---

## **Derived Metrics**

### 1. **Tokens per Millisecond Prompt (T/M Prompt)**
- **Definition**: The number of tokens in the prompt processed per millisecond.
- **Formula**:
  
  \[
  \text{T/M Prompt} = \frac{\text{Prompt Evaluation Count}}{\text{Prompt Evaluation Duration} \times 10^{-6}}
  \]
- **Purpose**: Evaluates the prompt processing efficiency.

### 2. **Latency Ratio**
- **Definition**: The ratio of model loading time to the total time.
- **Formula**:
  
  \[
  \text{Latency Ratio} = \frac{\text{Load Duration}}{\text{Total Duration}}
  \]
- **Purpose**: Highlights the impact of model loading on overall performance.

### 3. **Prompt-to-Response Time Ratio (P2R Ratio)**
- **Definition**: The ratio of time spent evaluating the prompt to the time spent generating the response.
- **Formula**:
  
  \[
  \text{P2R Ratio} = \frac{\text{Prompt Evaluation Duration}}{\text{Evaluation Duration}}
  \]
- **Purpose**: Analyzes the balance between input processing and response generation.

### 4. **Normalized Token Efficiency (NTE)**
- **Definition**: Tokens generated per unit of total time excluding model loading.
- **Formula**:
  
  \[
  \text{NTE} = \frac{\text{Evaluation Count}}{\text{Total Duration} - \text{Load Duration}}
  \]
- **Purpose**: Evaluates throughput efficiency after accounting for loading overhead.

### 5. **Prompt Complexity Efficiency (PCE)**
- **Definition**: The efficiency of processing prompt tokens per second.
- **Formula**:
  
  \[
  \text{PCE} = \frac{\text{Prompt Evaluation Count}}{\text{Prompt Evaluation Duration} \times 10^{-9}}
  \]
- **Purpose**: Assesses how well the model handles complex or lengthy prompts.

### 6. **Model Preparation Overhead (MPO)**
- **Definition**: The proportion of total time spent on non-prompt and non-response tasks (e.g., model loading).
- **Formula**:
  
  \[
  \text{MPO} = \frac{\text{Total Duration} - (\text{Prompt Evaluation Duration} + \text{Evaluation Duration})}{\text{Total Duration}}
  \]
- **Purpose**: Identifies inefficiencies unrelated to core processing.

### 7. **Token Cost Efficiency (TCE)**
- **Definition**: The number of response tokens generated per input prompt token.
- **Formula**:
  
  \[
  \text{TCE} = \frac{\text{Evaluation Count}}{\text{Prompt Evaluation Count}}
  \]
- **Purpose**: Reflects the output efficiency relative to the input size.

### 8. **Response Time Per Token (RTT)**
- **Definition**: The average time taken to generate each response token.
- **Formula**:
  
  \[
  \text{RTT} = \frac{\text{Evaluation Duration}}{\text{Evaluation Count}}
  \]
- **Purpose**: Identifies the time complexity of token generation.

### 9. **Idle Time Ratio (ITR)**
- **Definition**: The proportion of time spent on tasks unrelated to prompt evaluation and response generation.
- **Formula**:
  
  \[
  \text{ITR} = \frac{\text{Total Duration} - (\text{Prompt Evaluation Duration} + \text{Evaluation Duration})}{\text{Total Duration}}
  \]
- **Purpose**: Highlights inefficiencies in task allocation.

### 10. **Prompt Utilization Efficiency (PUE)**
- **Definition**: Tokens generated per second of prompt evaluation.
- **Formula**:
  
  \[
  \text{PUE} = \frac{\text{Evaluation Count}}{\text{Prompt Evaluation Duration} \times 10^{-9}}
  \]
- **Purpose**: Tracks how effectively the prompt contributes to response generation.

### 11. **Scaling Factor Efficiency (SFE)**
- **Definition**: Measures the modelâ€™s ability to handle increased prompt and response sizes efficiently.
- **Formula**:
  
  \[
  \text{SFE} = \frac{\text{Evaluation Count}}{\text{Prompt Evaluation Count} + \text{Evaluation Count}}
  \]
- **Purpose**: Evaluates scalability when handling large inputs or outputs.

---

## **Usage**
These metrics provide a multi-faceted approach to evaluate LLM performance. By combining existing and novel metrics, users can:

1. **Identify Bottlenecks**: Pinpoint areas where the model or system can be optimized.
2. **Benchmark Models**: Compare different models or configurations effectively.
3. **Optimize Workflows**: Improve the balance between prompt processing and response generation.

These metrics can be applied in offline or online scenarios, ensuring robust and comprehensive evaluation of LLMs.

